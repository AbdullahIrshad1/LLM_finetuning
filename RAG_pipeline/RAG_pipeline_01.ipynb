{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“š LLM-powered RAG chatbot using LangChain, Ollama, and ChromaDB for local document querying.\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project implements a **Retrieval-Augmented Generation (RAG) Chatbot** that allows users to ask questions based on a custom uploaded text file (e.g., `model.txt`). It leverages the power of large language models (LLMs) with local context retrieval to produce grounded, document-specific answers. The pipeline consists of preprocessing the input document, indexing it with vector embeddings, and querying it with a user-friendly Gradio web interface.\n",
        "\n",
        "##  Key Features\n",
        "\n",
        "- **Upload or Preload Document**: Supports loading a static file or user-uploaded `.txt` documents.\n",
        "- **Embeddings and Retrieval**: Converts the text into vector embeddings using OpenAI or HuggingFace-compatible models and performs semantic similarity search to fetch relevant chunks.\n",
        "- **LLM-Powered Answers**: Uses a language model to generate answers based on retrieved chunks, making it more accurate and context-aware than a vanilla chatbot.\n",
        "- **Clean & Generic Output**: Strips out all system-level or internal model artifacts like `<think>`, `<|im_start|>`, etc., ensuring clean, readable answers.\n",
        "- **Minimalist Gradio UI**: A responsive interface where users can input questions and receive instant, relevant answers.\n",
        "\n",
        "##  Technologies Used\n",
        "\n",
        "- `LangChain`: For chaining vector database retrieval with LLM responses.\n",
        "- `FAISS`: For fast local vector search over the document.\n",
        "- `OpenAI` or `HuggingFace` models: To create embeddings and answer queries.\n",
        "- `Gradio`: For creating a sleek, no-code web interface.\n",
        "- `Python`: Core language for scripting, processing, and logic handling.\n",
        "\n",
        "##  Use Cases\n",
        "\n",
        "- Summarizing or querying academic research papers.\n",
        "- Understanding product manuals or documentation.\n",
        "- Interactive Q&A over any domain-specific content.\n",
        "- Lightweight alternative to full-fledged AI search engines with controllable context.\n",
        "\n",
        "##  How It Works\n",
        "\n",
        "1. **Preprocess Document**: The input text file is split into overlapping chunks for better retrieval granularity.\n",
        "2. **Embed and Store**: Each chunk is converted to vector form and stored in a FAISS index.\n",
        "3. **User Query**: The user asks a question via the UI.\n",
        "4. **Retrieve + Answer**: The most relevant text chunks are retrieved and passed to an LLM, which generates a clean response.\n",
        "5. **Return Clean Result**: All internal tokens and artifacts are stripped before displaying to the user.\n",
        "\n",
        "---\n",
        "\n",
        "This project demonstrates how you can integrate traditional document processing with modern LLM-powered natural language interfaces, creating a powerful tool for contextual knowledge extraction.\n"
      ],
      "metadata": {
        "id": "dJQu0v_fIKlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install Required Libraries (If Needed)**"
      ],
      "metadata": {
        "id": "M61QTxWdEbQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb gradio\n"
      ],
      "metadata": {
        "id": "8dOotVJ_Efpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Split the Document**\n",
        "\n",
        "Load a .txt file and break it into manageable chunks for embedding and retrieval."
      ],
      "metadata": {
        "id": "9VXXhew9EiNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = TextLoader(\"model.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n"
      ],
      "metadata": {
        "id": "X75sVBmUEwCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Embeddings and Vectorstore**\n",
        "\n",
        "Convert document chunks into vector embeddings and store them in a Chroma vector database."
      ],
      "metadata": {
        "id": "oQB3UV9RE3om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OllamaEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "vectorstore = Chroma.from_documents(docs, embeddings)\n"
      ],
      "metadata": {
        "id": "2_FemhOiE7Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Up Retriever and LLM**\n",
        "\n",
        "Configure the retriever to search relevant document chunks and use an LLM to generate answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "pvGr8mUXE9hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import Ollama\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "llm = Ollama(model=\"deepseek-r1:1.5b-qwen-distill-fp16\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "yadlEnUVFB29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Chatbot Logic**\n",
        "\n",
        "Create a function to handle user input and return context-based responses."
      ],
      "metadata": {
        "id": "iPXSayAjFHle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(query):\n",
        "    response = qa_chain.run(query)\n",
        "\n",
        "    # Clean up LLM artifacts and generic tags\n",
        "    if isinstance(response, str):\n",
        "        cleaned = response.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").replace(\"<think>\", \"\").strip()\n",
        "        return cleaned if cleaned else \"Sorry, I couldnâ€™t find an answer to that.\"\n",
        "    return \"Sorry, I couldnâ€™t process your question.\"\n"
      ],
      "metadata": {
        "id": "jg075hapFTFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build and Launch Gradio Interface**\n",
        "\n",
        "Create a user-friendly web interface for chatting with your local document-powered assistant."
      ],
      "metadata": {
        "id": "ywne9VrbH3Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about model.txt...\"),\n",
        "    outputs=gr.Textbox(label=\"Response\"),\n",
        "    title=\"ðŸ“š Local RAG Chatbot\",\n",
        "    description=\"Interact with a Retrieval-Augmented Generation (RAG) chatbot that answers based on your uploaded document.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "sbumnn2_H75F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wXFEm9RkFT4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NukuzT6TEmTA"
      }
    }
  ]
}